{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emo_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi9716/Facial-Emotion-Recognition/blob/master/emo_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-vQ-o81_wrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kvu4zzPA_JI",
        "colab_type": "code",
        "outputId": "174082ee-9c62-4381-8e76-21e7fa276048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqZT-8bVA_sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.load(\"drive/My Drive/emo_data/X_train.npy\")\n",
        "\n",
        "X_test = np.load(\"drive/My Drive/emo_data/X_test.npy\")\n",
        "\n",
        "y_train = np.load(\"drive/My Drive/emo_data/y_train.npy\")\n",
        "y_train=np.asarray([np.argmax(y_train[i]) for i in range(len(y_train))])\n",
        "y_test = np.load(\"drive/My Drive/emo_data/y_test.npy\")\n",
        "y_test=np.asarray([np.argmax(y_test[i]) for i in range(len(y_test))])\n",
        "\n",
        "\n",
        "# Reshape data back to images, transpose to N,C,H,W format for pytorch.\n",
        "# X_train = X_train.transpose((0,3,1,2))\n",
        "# X_test = X_test.transpose((0,3,1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz_RFaYhcYlK",
        "colab_type": "code",
        "outputId": "6eae0d4f-c1f8-4c3a-bebd-35ba4012af88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EbTReCqBbuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    \n",
        "    #initialise the class variables - transform, data, target\n",
        "    def __init__(self, data, target, transform=None): \n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "        # converting target to torch.LongTensor dtype\n",
        "        self.target = torch.from_numpy(target).long() \n",
        "    \n",
        "    #retrieve the X and y index value and return it\n",
        "    def __getitem__(self, index): \n",
        "        return self.transform(self.data[index]), self.target[index]\n",
        "    \n",
        "    #returns the length of the data\n",
        "    def __len__(self): \n",
        "        return len(list(self.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSAnJPe4OYEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform= transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(), # Horizontal Flip\n",
        "            transforms.RandomCrop(44, padding=2), # Centre Crop\n",
        "            \n",
        "    transforms.ToTensor(),  #Convereting the input to tensor\n",
        "            ])\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToPILImage(), transforms.ToTensor()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KxbYwusBuRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class MyDataset(Dataset):\n",
        "    \n",
        "#     def __init__(self, x, y=None):\n",
        "#         self.data = x\n",
        "#         self.labels = y\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return self.data.shape[0]\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         if self.labels is not None:\n",
        "            \n",
        "#             return self.data[idx], self.labels[idx]\n",
        "#         else:\n",
        "#             return self.data[idx]\n",
        "          \n",
        "train_dataset = MyDataset(X_train, y_train,train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=252, shuffle=True)\n",
        "\n",
        "val_dataset = MyDataset(X_test, y_test,transform)\n",
        "test_loader = DataLoader(val_dataset, batch_size=252, shuffle=True)\n",
        "\n",
        "\n",
        "dataloaders = {'train' : train_loader, 'val' : test_loader }\n",
        "dataset_sizes = {'train' : len(X_train), 'val' : len(X_test) }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waZRxT4aCjDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8VPlqdGsZrY",
        "colab_type": "text"
      },
      "source": [
        "## Xception Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbuUbQQ6fLOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernel_size, padding, bias=False):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCAZiDxCfLVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Xception(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes=10):\n",
        "        super(Xception, self).__init__()\n",
        "        \n",
        "        # Entry Flow\n",
        "        self.entry_flow_1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_2 = nn.Sequential(\n",
        "            depthwise_separable_conv(64, 128, 3, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            depthwise_separable_conv(128, 128, 3, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_2_residual = nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0)\n",
        "        \n",
        "        self.entry_flow_3 = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(128, 256, 3, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(256, 256, 3, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_3_residual = nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0)\n",
        "        \n",
        "        self.entry_flow_4 = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(256, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        \n",
        "        self.entry_flow_4_residual = nn.Conv2d(256, 728, kernel_size=1, stride=2, padding=0)\n",
        "        \n",
        "        # Middle Flow\n",
        "        self.middle_flow = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728)\n",
        "        )\n",
        "        \n",
        "        # Exit Flow\n",
        "        self.exit_flow_1 = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 728, 3, 1),\n",
        "            nn.BatchNorm2d(728),\n",
        "            \n",
        "            nn.ReLU(True),\n",
        "            depthwise_separable_conv(728, 1024, 3, 1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            \n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "        self.exit_flow_1_residual = nn.Conv2d(728, 1024, kernel_size=1, stride=2, padding=0)\n",
        "        self.exit_flow_2 = nn.Sequential(\n",
        "            depthwise_separable_conv(1024, 1536, 3, 1),\n",
        "            nn.BatchNorm2d(1536),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            depthwise_separable_conv(1536, 2048, 3, 1),\n",
        "            nn.BatchNorm2d(2048),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        self.linear = nn.Linear(2048, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        entry_out1 = self.entry_flow_1(x)\n",
        "        entry_out2 = self.entry_flow_2(entry_out1) + self.entry_flow_2_residual(entry_out1)\n",
        "        entry_out3 = self.entry_flow_3(entry_out2) + self.entry_flow_3_residual(entry_out2)\n",
        "        entry_out = self.entry_flow_4(entry_out3) + self.entry_flow_4_residual(entry_out3)\n",
        "        \n",
        "        middle_out = self.middle_flow(entry_out) + entry_out\n",
        "\n",
        "        exit_out1 = self.exit_flow_1(middle_out) + self.exit_flow_1_residual(middle_out)\n",
        "        exit_out2 = self.exit_flow_2(exit_out1)\n",
        "\n",
        "        exit_avg_pool = F.adaptive_avg_pool2d(exit_out2, (1, 1))                \n",
        "        exit_avg_pool_flat = exit_avg_pool.view(exit_avg_pool.size(0), -1)\n",
        "\n",
        "        output = self.linear(exit_avg_pool_flat)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MksFttBSfLcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Xception(1, 7) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHxa2UQIfn3X",
        "colab_type": "code",
        "outputId": "39cc2198-2fd0-4df4-f386-614697c888e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Xception(\n",
              "  (entry_flow_1): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace)\n",
              "  )\n",
              "  (entry_flow_2): Sequential(\n",
              "    (0): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "      (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "      (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (entry_flow_2_residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (entry_flow_3): Sequential(\n",
              "    (0): ReLU(inplace)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "      (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "      (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (entry_flow_3_residual): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (entry_flow_4): Sequential(\n",
              "    (0): ReLU(inplace)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "      (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (entry_flow_4_residual): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (middle_flow): Sequential(\n",
              "    (0): ReLU(inplace)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace)\n",
              "    (7): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (exit_flow_1): Sequential(\n",
              "    (0): ReLU(inplace)\n",
              "    (1): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
              "      (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (exit_flow_1_residual): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
              "  (exit_flow_2): Sequential(\n",
              "    (0): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
              "      (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): depthwise_separable_conv(\n",
              "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
              "      (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace)\n",
              "  )\n",
              "  (linear): Linear(in_features=2048, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEAQbFy4fn5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights=torch.FloatTensor(np.bincount(y_train)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HOq7uISfn71",
        "colab_type": "code",
        "outputId": "c7fc39c8-336c-4a64-a210-4065fa861d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_ft = train_model(net, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.5979 Acc: 0.3635\n",
            "val Loss: 1.4607 Acc: 0.4090\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.3397 Acc: 0.4856\n",
            "val Loss: 1.3463 Acc: 0.4893\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.2279 Acc: 0.5294\n",
            "val Loss: 1.2818 Acc: 0.5082\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.1611 Acc: 0.5590\n",
            "val Loss: 1.2617 Acc: 0.5216\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.1187 Acc: 0.5735\n",
            "val Loss: 1.2097 Acc: 0.5444\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.0817 Acc: 0.5916\n",
            "val Loss: 1.1370 Acc: 0.5698\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.0045 Acc: 0.6201\n",
            "val Loss: 1.0657 Acc: 0.6010\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.9796 Acc: 0.6302\n",
            "val Loss: 1.0559 Acc: 0.6035\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.9599 Acc: 0.6372\n",
            "val Loss: 1.0552 Acc: 0.6049\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.9428 Acc: 0.6475\n",
            "val Loss: 1.0531 Acc: 0.6082\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.9332 Acc: 0.6487\n",
            "val Loss: 1.0427 Acc: 0.6099\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.9170 Acc: 0.6539\n",
            "val Loss: 1.0570 Acc: 0.6080\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.9084 Acc: 0.6592\n",
            "val Loss: 1.0392 Acc: 0.6158\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.8818 Acc: 0.6691\n",
            "val Loss: 1.0280 Acc: 0.6205\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.8779 Acc: 0.6721\n",
            "val Loss: 1.0296 Acc: 0.6205\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.8737 Acc: 0.6721\n",
            "val Loss: 1.0312 Acc: 0.6194\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.8681 Acc: 0.6740\n",
            "val Loss: 1.0301 Acc: 0.6180\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.8615 Acc: 0.6787\n",
            "val Loss: 1.0324 Acc: 0.6191\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.8616 Acc: 0.6757\n",
            "val Loss: 1.0287 Acc: 0.6199\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.8579 Acc: 0.6796\n",
            "val Loss: 1.0317 Acc: 0.6219\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.8525 Acc: 0.6818\n",
            "val Loss: 1.0315 Acc: 0.6172\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.8518 Acc: 0.6817\n",
            "val Loss: 1.0304 Acc: 0.6222\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.8507 Acc: 0.6820\n",
            "val Loss: 1.0312 Acc: 0.6208\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.8470 Acc: 0.6823\n",
            "val Loss: 1.0308 Acc: 0.6191\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.8486 Acc: 0.6813\n",
            "val Loss: 1.0296 Acc: 0.6219\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.8486 Acc: 0.6827\n",
            "val Loss: 1.0322 Acc: 0.6183\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.8462 Acc: 0.6820\n",
            "val Loss: 1.0299 Acc: 0.6222\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.8510 Acc: 0.6795\n",
            "val Loss: 1.0318 Acc: 0.6225\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.8515 Acc: 0.6813\n",
            "val Loss: 1.0341 Acc: 0.6213\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.8487 Acc: 0.6846\n",
            "val Loss: 1.0305 Acc: 0.6227\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.8501 Acc: 0.6828\n",
            "val Loss: 1.0317 Acc: 0.6199\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.8468 Acc: 0.6836\n",
            "val Loss: 1.0316 Acc: 0.6205\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.8492 Acc: 0.6813\n",
            "val Loss: 1.0332 Acc: 0.6180\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.8450 Acc: 0.6841\n",
            "val Loss: 1.0294 Acc: 0.6227\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.8466 Acc: 0.6805\n",
            "val Loss: 1.0325 Acc: 0.6199\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.8489 Acc: 0.6817\n",
            "val Loss: 1.0321 Acc: 0.6208\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.8478 Acc: 0.6839\n",
            "val Loss: 1.0308 Acc: 0.6188\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.8475 Acc: 0.6835\n",
            "val Loss: 1.0314 Acc: 0.6202\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.8474 Acc: 0.6831\n",
            "val Loss: 1.0302 Acc: 0.6205\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.8482 Acc: 0.6817\n",
            "val Loss: 1.0322 Acc: 0.6211\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.8449 Acc: 0.6853\n",
            "val Loss: 1.0329 Acc: 0.6216\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.8441 Acc: 0.6824\n",
            "val Loss: 1.0321 Acc: 0.6199\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.8510 Acc: 0.6815\n",
            "val Loss: 1.0313 Acc: 0.6186\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.8475 Acc: 0.6817\n",
            "val Loss: 1.0300 Acc: 0.6208\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.8489 Acc: 0.6819\n",
            "val Loss: 1.0308 Acc: 0.6202\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.8432 Acc: 0.6846\n",
            "val Loss: 1.0325 Acc: 0.6191\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.8471 Acc: 0.6846\n",
            "val Loss: 1.0313 Acc: 0.6188\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.8475 Acc: 0.6816\n",
            "val Loss: 1.0321 Acc: 0.6219\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.8504 Acc: 0.6819\n",
            "val Loss: 1.0324 Acc: 0.6191\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.8493 Acc: 0.6813\n",
            "val Loss: 1.0323 Acc: 0.6219\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.8458 Acc: 0.6860\n",
            "val Loss: 1.0323 Acc: 0.6219\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.8491 Acc: 0.6814\n",
            "val Loss: 1.0302 Acc: 0.6213\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.8439 Acc: 0.6832\n",
            "val Loss: 1.0304 Acc: 0.6205\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.8430 Acc: 0.6865\n",
            "val Loss: 1.0312 Acc: 0.6219\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.8463 Acc: 0.6821\n",
            "val Loss: 1.0313 Acc: 0.6230\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.8468 Acc: 0.6818\n",
            "val Loss: 1.0298 Acc: 0.6222\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.8447 Acc: 0.6834\n",
            "val Loss: 1.0320 Acc: 0.6199\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.8501 Acc: 0.6823\n",
            "val Loss: 1.0294 Acc: 0.6216\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.8451 Acc: 0.6841\n",
            "val Loss: 1.0319 Acc: 0.6219\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.8438 Acc: 0.6844\n",
            "val Loss: 1.0300 Acc: 0.6197\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.8445 Acc: 0.6853\n",
            "val Loss: 1.0315 Acc: 0.6227\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.8439 Acc: 0.6824\n",
            "val Loss: 1.0293 Acc: 0.6222\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.8454 Acc: 0.6832\n",
            "val Loss: 1.0310 Acc: 0.6202\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.8466 Acc: 0.6824\n",
            "val Loss: 1.0308 Acc: 0.6219\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.8501 Acc: 0.6831\n",
            "val Loss: 1.0309 Acc: 0.6213\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.8419 Acc: 0.6876\n",
            "val Loss: 1.0322 Acc: 0.6194\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.8465 Acc: 0.6817\n",
            "val Loss: 1.0306 Acc: 0.6197\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.8431 Acc: 0.6816\n",
            "val Loss: 1.0304 Acc: 0.6225\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.8477 Acc: 0.6837\n",
            "val Loss: 1.0301 Acc: 0.6211\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.8447 Acc: 0.6834\n",
            "val Loss: 1.0310 Acc: 0.6211\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.8498 Acc: 0.6833\n",
            "val Loss: 1.0340 Acc: 0.6194\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.8440 Acc: 0.6832\n",
            "val Loss: 1.0314 Acc: 0.6211\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.8477 Acc: 0.6819\n",
            "val Loss: 1.0289 Acc: 0.6205\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.8467 Acc: 0.6853\n",
            "val Loss: 1.0319 Acc: 0.6191\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.8473 Acc: 0.6823\n",
            "val Loss: 1.0328 Acc: 0.6216\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.8501 Acc: 0.6814\n",
            "val Loss: 1.0290 Acc: 0.6213\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.8437 Acc: 0.6855\n",
            "val Loss: 1.0316 Acc: 0.6202\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.8493 Acc: 0.6824\n",
            "val Loss: 1.0315 Acc: 0.6225\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.8464 Acc: 0.6840\n",
            "val Loss: 1.0313 Acc: 0.6208\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.8426 Acc: 0.6819\n",
            "val Loss: 1.0303 Acc: 0.6194\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.8497 Acc: 0.6823\n",
            "val Loss: 1.0316 Acc: 0.6219\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.8464 Acc: 0.6825\n",
            "val Loss: 1.0306 Acc: 0.6219\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.8448 Acc: 0.6852\n",
            "val Loss: 1.0323 Acc: 0.6186\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.8476 Acc: 0.6836\n",
            "val Loss: 1.0297 Acc: 0.6225\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.8469 Acc: 0.6815\n",
            "val Loss: 1.0327 Acc: 0.6211\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.8461 Acc: 0.6850\n",
            "val Loss: 1.0300 Acc: 0.6219\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.8456 Acc: 0.6849\n",
            "val Loss: 1.0311 Acc: 0.6213\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.8482 Acc: 0.6823\n",
            "val Loss: 1.0324 Acc: 0.6219\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.8457 Acc: 0.6813\n",
            "val Loss: 1.0326 Acc: 0.6205\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.8503 Acc: 0.6815\n",
            "val Loss: 1.0330 Acc: 0.6197\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.8467 Acc: 0.6822\n",
            "val Loss: 1.0305 Acc: 0.6225\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.8485 Acc: 0.6801\n",
            "val Loss: 1.0295 Acc: 0.6211\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.8503 Acc: 0.6821\n",
            "val Loss: 1.0299 Acc: 0.6227\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.8463 Acc: 0.6822\n",
            "val Loss: 1.0310 Acc: 0.6191\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.8445 Acc: 0.6861\n",
            "val Loss: 1.0315 Acc: 0.6211\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.8491 Acc: 0.6821\n",
            "val Loss: 1.0330 Acc: 0.6180\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.8511 Acc: 0.6771\n",
            "val Loss: 1.0310 Acc: 0.6205\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.8478 Acc: 0.6836\n",
            "val Loss: 1.0295 Acc: 0.6211\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.8459 Acc: 0.6835\n",
            "val Loss: 1.0301 Acc: 0.6222\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.8462 Acc: 0.6819\n",
            "val Loss: 1.0313 Acc: 0.6219\n",
            "\n",
            "Training complete in 107m 29s\n",
            "Best val Acc: 0.623015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2fNfG0Nfn-J",
        "colab_type": "code",
        "outputId": "b1435344-76b0-42b5-abee-6baeac175669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "torch.save(net.state_dict(),'emo_model.pt')\n",
        "torch.save(net, 'emo_entire_model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Xception. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type depthwise_separable_conv. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnGrqWmUsC-I",
        "colab_type": "text"
      },
      "source": [
        "## Resnet18 Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVWcJ-mhEUm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_ft = models.resnet18(pretrained=False)\n",
        "# model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 7)\n",
        "\n",
        "# model_ft = model_ft.to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# # Observe that all parameters are being optimized\n",
        "# optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "# # Decay LR by a factor of 0.1 every 7 epochs\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cGhRlpjHNFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "#                        num_epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gK74iHFqGV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.save(model_ft.state_dict(),'emo_model.pt')\n",
        "# torch.save(model_ft, 'emo_entire_model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCnHtEkEqL30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}